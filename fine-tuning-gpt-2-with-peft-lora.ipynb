{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://i.postimg.cc/26Knx6B6/Screenshot-2025-08-11-113135.png)","metadata":{}},{"cell_type":"markdown","source":"<span style=\"color: white; background-color: red; padding: 10px 20px; border-radius: 10px; font-size: 36px; font-weight: bold;\">Install Dependencies</span>\n","metadata":{}},{"cell_type":"code","source":"#!pip install -q datasets transformers peft accelerate bitsandbytes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T05:43:26.407726Z","iopub.execute_input":"2025-08-11T05:43:26.408048Z","iopub.status.idle":"2025-08-11T05:43:26.412304Z","shell.execute_reply.started":"2025-08-11T05:43:26.408026Z","shell.execute_reply":"2025-08-11T05:43:26.411497Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"<span style=\"color: white; background-color: red; padding: 10px 20px; border-radius: 10px; font-size: 36px; font-weight: bold;\">Basic Imports</span>\n","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model\nimport torch\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T05:43:26.413350Z","iopub.execute_input":"2025-08-11T05:43:26.413572Z","iopub.status.idle":"2025-08-11T05:43:52.539818Z","shell.execute_reply.started":"2025-08-11T05:43:26.413557Z","shell.execute_reply":"2025-08-11T05:43:52.539218Z"}},"outputs":[{"name":"stderr","text":"2025-08-11 05:43:40.643163: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754891020.823068      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754891020.873562      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"<span style=\"color: white; background-color: red; padding: 10px 20px; border-radius: 10px; font-size: 36px; font-weight: bold;\">Load dataset & Tokenizer</span>\n","metadata":{}},{"cell_type":"code","source":"# Load dataset\ndataset = load_dataset(\"Abirate/english_quotes\")\n# Let's split 90% train / 10% validation without overlap\ndataset_split = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n\ntrain_data = dataset_split[\"train\"]\nval_data = dataset_split[\"test\"]\n\n# Load tokenizer\nmodel_name = \"gpt2\"\n# AutoTokenizer picks the right tokenizer class automatically based on the model \ntokenizer = AutoTokenizer.from_pretrained(model_name)\n# give GPT-2 a fake pad token so batching works during training\ntokenizer.pad_token = tokenizer.eos_token\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T05:43:52.540434Z","iopub.execute_input":"2025-08-11T05:43:52.540875Z","iopub.status.idle":"2025-08-11T05:44:05.230014Z","shell.execute_reply.started":"2025-08-11T05:43:52.540858Z","shell.execute_reply":"2025-08-11T05:44:05.229227Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4526b6f646040719c62a6a56166e7ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"quotes.jsonl:   0%|          | 0.00/647k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eff43b75a75147edaac1f86e13f1cd7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2508 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2994aa4ffa24c65a978054e01f8c65e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ef4ea69c6094629b825954e3be32784"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac32b7d1bb3a47b998b6289ad90f9f18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8be2082aae9b4986980a73c282738023"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2b81808ad2849b8be317a2087aa7c66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9a5aa6e35284b7b8687ca10f88e27a0"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"<span style=\"color: white; background-color: red; padding: 10px 20px; border-radius: 10px; font-size: 36px; font-weight: bold;\">Tokenize</span>\n","metadata":{}},{"cell_type":"code","source":"# Tokenize + add labels\ndef tokenize(batch):\n    tokenized = tokenizer(batch[\"quote\"], padding=\"max_length\", truncation=True, max_length=64)\n    # It shifts the labels internally by 1 position when calculating the loss\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n# batched = True -->> sends data in batches\ntrain_data = train_data.map(tokenize, batched=True)\nval_data = val_data.map(tokenize, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T05:44:05.231557Z","iopub.execute_input":"2025-08-11T05:44:05.231940Z","iopub.status.idle":"2025-08-11T05:44:05.590930Z","shell.execute_reply.started":"2025-08-11T05:44:05.231909Z","shell.execute_reply":"2025-08-11T05:44:05.590187Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2257 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e2ee6345ff94a028b22eb2f74373f6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/251 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2af450c21a8848528dab7e600b0e6794"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"<span style=\"color: white; background-color: red; padding: 10px 20px; border-radius: 10px; font-size: 36px; font-weight: bold;\">Load Model</span>\n","metadata":{}},{"cell_type":"code","source":"# Load model in FP16\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    # device map -->> If you have multiple GPUs, it can split layers between them.\n    device_map=\"auto\"\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T05:44:05.592063Z","iopub.execute_input":"2025-08-11T05:44:05.592355Z","iopub.status.idle":"2025-08-11T05:44:10.929240Z","shell.execute_reply.started":"2025-08-11T05:44:05.592335Z","shell.execute_reply":"2025-08-11T05:44:10.928675Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46b25550ecef4fd2b694413ab957b160"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd7047df4f45481c837430c523fa71c4"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"<span style=\"color: white; background-color: red; padding: 10px 20px; border-radius: 10px; font-size: 36px; font-weight: bold;\">LoRA config + Training arguments + training</span>\n","metadata":{}},{"cell_type":"markdown","source":"![](https://i.postimg.cc/qqyjFyrq/0-wwg-To6-O04-U50k4-LZ.png)","metadata":{}},{"cell_type":"code","source":"# LoRA config\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"c_attn\"],  # GPT-2 attention\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Inserts low-rank LoRA layers into those modules\nmodel = get_peft_model(model, lora_config)\n\n#  Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./lora-llm\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    # batch_size=4, grad_accum_steps=2 → effective batch size = 4 × 2 = 8\n    gradient_accumulation_steps=2,\n    eval_strategy=\"steps\",\n    eval_steps=20,\n    logging_steps=10,\n    save_steps=50,\n    learning_rate=2e-4,\n    num_train_epochs=5,\n    fp16=True,\n    report_to=\"none\"\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n    tokenizer=tokenizer\n)\n\n# Train\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T05:44:10.929833Z","iopub.execute_input":"2025-08-11T05:44:10.930019Z","iopub.status.idle":"2025-08-11T05:47:48.878203Z","shell.execute_reply.started":"2025-08-11T05:44:10.930003Z","shell.execute_reply":"2025-08-11T05:47:48.877278Z"},"_kg_hide-output":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n/tmp/ipykernel_36/1581417024.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1415' max='1415' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1415/1415 03:36, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>5.525800</td>\n      <td>5.293876</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.011900</td>\n      <td>2.208154</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.133500</td>\n      <td>1.882907</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.980700</td>\n      <td>1.755885</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.696000</td>\n      <td>1.636082</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.649900</td>\n      <td>1.573870</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.698800</td>\n      <td>1.525108</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.879200</td>\n      <td>1.509992</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.909400</td>\n      <td>1.504137</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.766500</td>\n      <td>1.499040</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.615600</td>\n      <td>1.493916</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.586400</td>\n      <td>1.489367</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.590800</td>\n      <td>1.481769</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.521000</td>\n      <td>1.479249</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.761200</td>\n      <td>1.475856</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.770300</td>\n      <td>1.475616</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.576800</td>\n      <td>1.478151</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.782600</td>\n      <td>1.474513</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.561600</td>\n      <td>1.472347</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.567700</td>\n      <td>1.470443</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.671400</td>\n      <td>1.467601</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.711700</td>\n      <td>1.467637</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>1.717700</td>\n      <td>1.467442</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>1.566900</td>\n      <td>1.467796</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.360600</td>\n      <td>1.466940</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>1.681500</td>\n      <td>1.463515</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>1.781800</td>\n      <td>1.463074</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>1.464200</td>\n      <td>1.462408</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>1.912600</td>\n      <td>1.460632</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.551400</td>\n      <td>1.460393</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>1.583000</td>\n      <td>1.460326</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>1.410400</td>\n      <td>1.459492</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>1.454600</td>\n      <td>1.457248</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>1.587600</td>\n      <td>1.457263</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.501000</td>\n      <td>1.458239</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>1.412900</td>\n      <td>1.457127</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>1.774300</td>\n      <td>1.455636</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>1.637500</td>\n      <td>1.454259</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>1.611200</td>\n      <td>1.454448</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.600100</td>\n      <td>1.454389</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>1.748100</td>\n      <td>1.452902</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>1.701400</td>\n      <td>1.453244</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>1.535700</td>\n      <td>1.453508</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>1.484700</td>\n      <td>1.453192</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.570300</td>\n      <td>1.452091</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>1.705200</td>\n      <td>1.451437</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>1.666400</td>\n      <td>1.451011</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>1.450300</td>\n      <td>1.451313</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>1.835900</td>\n      <td>1.451326</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.549300</td>\n      <td>1.451171</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>1.460100</td>\n      <td>1.451538</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>1.473600</td>\n      <td>1.450106</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>1.635100</td>\n      <td>1.449568</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>1.748700</td>\n      <td>1.449478</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>1.783600</td>\n      <td>1.449859</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>1.739000</td>\n      <td>1.449885</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>1.489400</td>\n      <td>1.449702</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>1.639400</td>\n      <td>1.448971</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>1.693500</td>\n      <td>1.449251</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.527600</td>\n      <td>1.449142</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>1.521800</td>\n      <td>1.449199</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>1.633200</td>\n      <td>1.449088</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>1.591000</td>\n      <td>1.449202</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>1.445600</td>\n      <td>1.448726</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>1.685800</td>\n      <td>1.448373</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>1.484800</td>\n      <td>1.448307</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>1.771600</td>\n      <td>1.448239</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>1.555600</td>\n      <td>1.447869</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>1.579000</td>\n      <td>1.447899</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.621200</td>\n      <td>1.447890</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1415, training_loss=1.7523778827788552, metrics={'train_runtime': 217.4738, 'train_samples_per_second': 51.891, 'train_steps_per_second': 6.507, 'total_flos': 369863056097280.0, 'train_loss': 1.7523778827788552, 'epoch': 5.0})"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"<span style=\"color: white; background-color: red; padding: 10px 20px; border-radius: 10px; font-size: 36px; font-weight: bold;\">Save the model</span>\n","metadata":{}},{"cell_type":"code","source":"#  Save LoRA adapter\nmodel.save_pretrained(\"lora-gpt2\")\ntokenizer.save_pretrained(\"lora-gpt2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T05:47:48.878977Z","iopub.execute_input":"2025-08-11T05:47:48.879199Z","iopub.status.idle":"2025-08-11T05:47:49.342129Z","shell.execute_reply.started":"2025-08-11T05:47:48.879180Z","shell.execute_reply":"2025-08-11T05:47:49.341364Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"('lora-gpt2/tokenizer_config.json',\n 'lora-gpt2/special_tokens_map.json',\n 'lora-gpt2/vocab.json',\n 'lora-gpt2/merges.txt',\n 'lora-gpt2/added_tokens.json',\n 'lora-gpt2/tokenizer.json')"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"<span style=\"color: white; background-color: red; padding: 10px 20px; border-radius: 10px; font-size: 36px; font-weight: bold;\">Inference From Saved Model</span>\n","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport torch\n\n#  Load tokenizer\nbase_model_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(\"lora-gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\n#  Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Load LoRA adapter\nmodel = PeftModel.from_pretrained(base_model, \"lora-gpt2\")\n\n#  Build pipeline\ntext_gen = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T05:47:49.342913Z","iopub.execute_input":"2025-08-11T05:47:49.343206Z","iopub.status.idle":"2025-08-11T05:47:50.614502Z","shell.execute_reply.started":"2025-08-11T05:47:49.343180Z","shell.execute_reply":"2025-08-11T05:47:50.613815Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Run inference\nprompt = \"The secret to happiness is\"\noutputs = text_gen(prompt, max_new_tokens=70, num_return_sequences=1, do_sample=True, temperature=0.7)\n\nprint(outputs[0][\"generated_text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T06:06:53.734344Z","iopub.execute_input":"2025-08-11T06:06:53.734642Z","iopub.status.idle":"2025-08-11T06:06:54.121900Z","shell.execute_reply.started":"2025-08-11T06:06:53.734620Z","shell.execute_reply":"2025-08-11T06:06:54.121124Z"}},"outputs":[{"name":"stdout","text":"The secret to happiness is not fear, but determination, determination and the willingness to work hard. You don't have to be a doctor to do this, but you can do it.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Run inference\nprompt = \"once upon a time\"\noutputs = text_gen(prompt, max_new_tokens=70, num_return_sequences=1, do_sample=True, temperature=0.7)\n\nprint(outputs[0][\"generated_text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T06:07:06.398852Z","iopub.execute_input":"2025-08-11T06:07:06.399158Z","iopub.status.idle":"2025-08-11T06:07:07.210727Z","shell.execute_reply.started":"2025-08-11T06:07:06.399137Z","shell.execute_reply":"2025-08-11T06:07:07.209882Z"}},"outputs":[{"name":"stdout","text":"once upon a time of great need, and when the world comes to an end, it is not to be feared. It is to be feared that we might have a better way of life in the future. And yet, our world could not be better. There was no hope. And when these things happen, it seems to us that we must not let them pass\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"<span style=\"color: white; background-color: red; padding: 10px 20px; border-radius: 10px; font-size: 36px; font-weight: bold;\">Smash That Upvote !! Thanks ! 🚀😃</span>\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null}]}