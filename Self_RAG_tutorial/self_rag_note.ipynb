{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61231f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "from prompts import DOCUMENT_GRADER_PROMPT, HALLUCINATION_GRADER_PROMPT, ANSWER_GRADER_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b3378d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables (Groq API key, etc.)\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5ca8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge base sources\n",
    "SOURCE_LINKS = [\n",
    "    \"https://www.geeksforgeeks.org/dsa/disjoint-set-data-structures/\",\n",
    "    \"https://en.wikipedia.org/wiki/Data_science\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e84b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared state for our workflow\n",
    "class WorkflowState(TypedDict):\n",
    "    user_question: str\n",
    "    answer_draft: str\n",
    "    retrieved_docs: List[str]\n",
    "    llm_model: ChatGroq\n",
    "    retriever: Chroma\n",
    "    has_hallucination: bool\n",
    "    is_valid_answer: bool\n",
    "\n",
    "\n",
    "# Models for grading\n",
    "class DocRelevanceScore(BaseModel):\n",
    "    binary_score: str = Field(description=\"'yes' if document is relevant, otherwise 'no'\")\n",
    "\n",
    "\n",
    "class HallucinationScore(BaseModel):\n",
    "    binary_score: str = Field(description=\"'yes' if grounded in facts, otherwise 'no'\")\n",
    "\n",
    "\n",
    "class AnswerValidityScore(BaseModel):\n",
    "    binary_score: str = Field(description=\"'yes' if answer addresses the question, otherwise 'no'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "363693db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Groq model\n",
    "def init_groq_model(state: WorkflowState) -> WorkflowState:\n",
    "    print(\"---LOADING GROQ MODEL---\")\n",
    "    state[\"llm_model\"] = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY, temperature=0)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3603982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create vector database\n",
    "def prepare_vector_database(state: WorkflowState) -> WorkflowState:\n",
    "    print(\"---CREATING VECTOR DATABASE---\")\n",
    "    all_docs = [WebBaseLoader(url).load() for url in SOURCE_LINKS]\n",
    "    flat_docs = [doc for group in all_docs for doc in group]\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=0)\n",
    "    split_docs = splitter.split_documents(flat_docs)\n",
    "\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=split_docs,\n",
    "        collection_name=\"custom_rag_store\",\n",
    "        embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"),\n",
    "    )\n",
    "    state[\"retriever\"] = vector_db.as_retriever()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50108c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Retrieve documents\n",
    "def fetch_relevant_docs(state: WorkflowState) -> WorkflowState:\n",
    "    print(\"---RETRIEVING RELEVANT DOCUMENTS---\")\n",
    "    state[\"retrieved_docs\"] = state[\"retriever\"].invoke(state[\"user_question\"])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da688ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Filter out irrelevant documents\n",
    "def filter_docs_by_relevance(state: WorkflowState) -> WorkflowState:\n",
    "    print(\"---FILTERING DOCUMENTS FOR RELEVANCE---\")\n",
    "    grader = state[\"llm_model\"].with_structured_output(DocRelevanceScore)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", DOCUMENT_GRADER_PROMPT),\n",
    "        (\"human\", \"Document: {document}\\n\\nQuestion: {question}\")\n",
    "    ])\n",
    "    evaluation_chain = prompt | grader\n",
    "\n",
    "    filtered = []\n",
    "    for doc in state[\"retrieved_docs\"]:\n",
    "        score = evaluation_chain.invoke({\"document\": doc.page_content, \"question\": state[\"user_question\"]})\n",
    "        if score.binary_score.lower() == \"yes\":\n",
    "            filtered.append(doc)\n",
    "\n",
    "    state[\"retrieved_docs\"] = filtered\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c87c492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Decide whether to answer or stop\n",
    "def should_generate_answer(state: WorkflowState) -> str:\n",
    "    print(\"---DECIDING NEXT STEP---\")\n",
    "    return \"answer\" if state[\"retrieved_docs\"] else \"stop\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45a89d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Generate answer\n",
    "def produce_answer(state: WorkflowState) -> WorkflowState:\n",
    "    print(\"---GENERATING ANSWER---\")\n",
    "    prompt_template = hub.pull(\"rlm/rag-prompt\")\n",
    "    rag_chain = prompt_template | state[\"llm_model\"] | StrOutputParser()\n",
    "    state[\"answer_draft\"] = rag_chain.invoke({\n",
    "        \"context\": state[\"retrieved_docs\"],\n",
    "        \"question\": state[\"user_question\"]\n",
    "    })\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88c27d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Check for hallucinations\n",
    "def detect_hallucination(state: WorkflowState) -> WorkflowState:\n",
    "    print(\"---CHECKING FOR HALLUCINATIONS---\")\n",
    "    grader = state[\"llm_model\"].with_structured_output(HallucinationScore)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", HALLUCINATION_GRADER_PROMPT),\n",
    "        (\"human\", \"Facts: {documents}\\n\\nGenerated Answer: {generation}\")\n",
    "    ])\n",
    "    chain = prompt | grader\n",
    "    result = chain.invoke({\n",
    "        \"documents\": state[\"retrieved_docs\"],\n",
    "        \"generation\": state[\"answer_draft\"]\n",
    "    })\n",
    "    state[\"has_hallucination\"] = (result.binary_score.lower() != \"yes\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "121aceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Grade final answer\n",
    "def validate_answer(state: WorkflowState) -> WorkflowState:\n",
    "    print(\"---VALIDATING ANSWER---\")\n",
    "    grader = state[\"llm_model\"].with_structured_output(AnswerValidityScore)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", ANSWER_GRADER_PROMPT),\n",
    "        (\"human\", \"Question: {question}\\n\\nAnswer: {generation}\")\n",
    "    ])\n",
    "    chain = prompt | grader\n",
    "    result = chain.invoke({\n",
    "        \"question\": state[\"user_question\"],\n",
    "        \"generation\": state[\"answer_draft\"]\n",
    "    })\n",
    "    state[\"is_valid_answer\"] = (result.binary_score.lower() == \"yes\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74a942ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Build graph\n",
    "def create_workflow():\n",
    "    workflow = StateGraph(WorkflowState)\n",
    "\n",
    "    workflow.add_node(\"init_groq_model\", init_groq_model)\n",
    "    workflow.add_node(\"prepare_vector_database\", prepare_vector_database)\n",
    "    workflow.add_node(\"fetch_relevant_docs\", fetch_relevant_docs)\n",
    "    workflow.add_node(\"filter_docs_by_relevance\", filter_docs_by_relevance)\n",
    "    workflow.add_node(\"produce_answer\", produce_answer)\n",
    "    workflow.add_node(\"detect_hallucination\", detect_hallucination)\n",
    "    workflow.add_node(\"validate_answer\", validate_answer)\n",
    "\n",
    "    workflow.add_edge(START, \"init_groq_model\")\n",
    "    workflow.add_edge(\"init_groq_model\", \"prepare_vector_database\")\n",
    "    workflow.add_edge(\"prepare_vector_database\", \"fetch_relevant_docs\")\n",
    "    workflow.add_edge(\"fetch_relevant_docs\", \"filter_docs_by_relevance\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"filter_docs_by_relevance\",\n",
    "        should_generate_answer,\n",
    "        {\"answer\": \"produce_answer\", \"stop\": END}\n",
    "    )\n",
    "    workflow.add_edge(\"produce_answer\", \"detect_hallucination\")\n",
    "    workflow.add_edge(\"detect_hallucination\", \"validate_answer\")\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4df99310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---LOADING GROQ MODEL---\n",
      "---CREATING VECTOR DATABASE---\n",
      "---RETRIEVING RELEVANT DOCUMENTS---\n",
      "---FILTERING DOCUMENTS FOR RELEVANCE---\n",
      "---DECIDING NEXT STEP---\n",
      "\n",
      "---FINAL OUTPUT---\n",
      " Warning: The answer may not fully address the question.\n",
      "\n",
      "Answer:\n",
      " No answer generated.\n"
     ]
    }
   ],
   "source": [
    "# Run workflow\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = create_workflow()\n",
    "    result = pipeline.invoke({\"user_question\": \"who is elon musk?\"})\n",
    "    \n",
    "    # Draw the workflow graph\n",
    "    graph = pipeline.get_graph()\n",
    "    graph.draw_mermaid_png(output_file_path=\"workflow_graph.png\")\n",
    "\n",
    "    print(\"\\n---FINAL OUTPUT---\")\n",
    "    if result.get(\"has_hallucination\"):\n",
    "        print(\" Warning: The answer may contain hallucinations.\\n\")\n",
    "    if not result.get(\"is_valid_answer\"):\n",
    "        print(\" Warning: The answer may not fully address the question.\\n\")\n",
    "    print(\"Answer:\\n\", result.get(\"answer_draft\", \"No answer generated.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d1341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
